from PIL import Image 
import numpy as np

# Nedded for background fit
import numpy.polynomial.polynomial as poly
# needed for spotfinder
import scipy.ndimage as ndimage
import scipy.ndimage.filters as filters


#############
# The PE vals looks in a raw tiff file, it readin all of the symbols and 
# splits where the info about the camara settings are
# it returns 2 values that are used in converting the adc counts to PE
#############
def PE_Vals(FILE):
    with open(FILE, encoding="utf8", errors='ignore') as f:
        contents = f.read()
    contents = contents.split("Created by Hamamatsu Inc.")[1].split('\n')
    for line in contents:
        if 'eOffset1' in line:
            eOffset = float(line.split("eOffset1 = ")[1])
        elif 'eCoeff1' in line:
            eCoeff = float(line.split("eCoeff1 = ")[1])
    return eOffset, eCoeff

#############
# This takes the raw tiff file and crops it to the proper shape and converts
# the adc counts into PE
#############
def Image_Converter(FILE,eOffset,eCoeff,Xindex,Yindex,Row,Col):
    TestImage = Image.open(FILE)
    Testspot = np.array(TestImage)[Yindex-Row:Yindex+Row+1,Xindex-Col:Xindex+Col+1]
    Shape = Testspot.shape[0]
    eCoeffM = eCoeff*np.ones(Shape**2).reshape((Shape,Shape))
    eOffsetM = eOffset*np.ones(Shape**2).reshape((Shape,Shape))
    Testspot = eCoeffM*(Testspot - eOffsetM)
    TestImage.close()
    return Testspot




#################################################################
# datafiles is the list of images in the stack ( should be in order)
# Shape is the shape of the image 
# eCoeffMatrix / eOffsetMatrix are generated from Shaper above
# Frac is the percent of the images at the end of the stack you want to 
# use to generate the background profile 
#################################################################
def Image_Background(h5file,Frame_Name, Num_Frames, Shape, Frac, PolyOrder):
    xvs = np.arange(0,Shape)
    MeanFit = np.zeros(Shape**2).reshape((Shape,Shape))
    LL = Num_Frames
    Fraction = int(Num_Frames*Frac)
    for x in range(Num_Frames-Fraction,Num_Frames):
        Images = h5file.root.ImageStack[Frame_Name[x]].read()
        ImageFit = []
        for y in range(0,Shape):
            coefs = poly.polyfit(xvs,Images[y], PolyOrder) 
            ffit  = poly.polyval(xvs, coefs)
            ImageFit.append(ffit)
        MeanFit+=np.array(ImageFit)
    MeanFit = MeanFit/(Fraction)
    return MeanFit




#################################################################
# datafiles is the list of images in the stack ( should be in order)
# EmptyData is the empty matrix generated by Shaper 
# eCoeffMatrix / eOffsetMatrix are generated from Shaper above
# MeanFit is generated above 
# 
# This returns the summed stack where each image is background corrected 
# this is needed to find the high points with the next function
#################################################################
def Summed_Image(h5file, Frame_Name, Num_Frames, Shape, MeanFit):
    Summed = np.zeros(Shape**2).reshape((Shape,Shape))
    for q in range(0,Num_Frames):
        Images = h5file.root.ImageStack[Frame_Name[q]].read()
        Images = Images - MeanFit 
        Summed += Images
    return Summed



#################################################################
# datafiles is the list of images in the stack ( should be in order)
# This function scans the data and finds maxima in a region that is the size of 
# neighborhood_size. It looks for points in the neighborhood_size that are above
# the threshold. The threshold is defined as the mean of the data set + threshold_sigma
# deavations above the mean. the lower the threshold_sigma the more points you will get.
# also lowering the neighborhood_size will yeild more points
#
# Shape and EdgeCut are included so you can cut points out that are near the edge of the image
#################################################################
def Spot_finder(SummedData, neighborhood_size, threshold_sigma, Shape, EdgeCut):

    threshold = np.mean(SummedData)+threshold_sigma*np.std(SummedData)
    data_max = filters.maximum_filter(SummedData, neighborhood_size)
    maxima = (SummedData == data_max)
    data_min = filters.minimum_filter(SummedData, neighborhood_size)
    diff = ((data_max - data_min) > threshold)
    maxima[diff == 0] = 0

    labeled, num_objects = ndimage.label(maxima)
    slices = ndimage.find_objects(labeled)
    AllPairs = []
    for dy,dx in slices:
        x_center = (dx.start + dx.stop - 1)/2
        y_center = (dy.start + dy.stop - 1)/2
        if x_center>EdgeCut and x_center<Shape-EdgeCut and y_center>EdgeCut and y_center<Shape-EdgeCut:
            AllPairs.append([x_center,y_center])
    return np.array(AllPairs)



#################################################################
# Takes the datafiles and the IDed points and computes the area for each point
# through the whole stack
#################################################################
def Spot_Area(h5file, Frame_Name, Num_Frames, MeanFit, All):
    SpotInfo = []
    for q in range(0,Num_Frames):
        #Images = (np.array(Image.open(datafiles[q])) )
        #Images = eCoeffMatrix*(Images - eOffsetMatrix)-MeanFit# is this needed?
        Images = h5file.root.ImageStack[Frame_Name[q]].read()
        Images = Images - MeanFit # is this needed?
        Images[Images < 0] = 0 

        Row = int(4) # define the area to take around the spots
        Col = int(4) # define the area to take around the spots
        for we in range(0,len(All)):
            Xindex = int(All[we][0])
            Yindex = int(All[we][1])

            spot1 = np.array(Images[Yindex-Row:Yindex+Row+1,Xindex-Col:Xindex+Col+1])
            spot2 = np.array(Images[Yindex-Row:Yindex+Row+1,Xindex-Col:Xindex+Col+1])
            spot = spot1[2:7,2:7]
            SpotRemove = np.zeros(5**2).reshape((5,5))
            spot2[2:7,2:7]=SpotRemove
            spotB = spot2

            ATot = spot.sum()
            ATotB = spot2.sum()
            SpotInfo.append([Xindex,Yindex,ATot,ATotB])
    return SpotInfo


def Step_fit(Time, Signal, Sigma):
    ch = []
    for x in range(1,len(Time)-1):
        DataLeft  = Signal[:x]
        DataRight = Signal[x:]
        sigmasLeft  = Sigma[:x]
        sigmasRight = Sigma[x:]
        mean  = np.mean(Signal)
        meanI  = np.mean(DataLeft)
        meanE  = np.mean(DataRight)

        chiHIGH = np.sum((DataLeft - meanI)**2/abs(sigmasLeft)**2)/749
        chiLOW = np.sum((DataRight - meanE)**2/abs(sigmasRight)**2)/749
        cc = chiHIGH+chiLOW
        ch.append(cc)
    chi = min(ch)
    loc = np.where(ch==chi)[0][0]+1 #+1 for the loop offset
    MeanDiff = (np.mean(Signal[:loc])) - ((np.mean(Signal[loc:])))
    return MeanDiff, chi, loc


def Line_fit(Time, Signal, Sigma):
    coefs=np.polyfit(Time,Signal,1)    
    ffit  = coefs[0]*Time+coefs[1]
    chiFit  = np.sum((ffit - Signal)**2/abs(Sigma)**2)/748
    return chiFit